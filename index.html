
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>ICMI 2021 Workshop on Multimodal Learning with Fewer Samples</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link href="css/style.css" rel="stylesheet" type="text/css"/>
</head>

<body>

<div class="container">
    <table border="0" align="center">
        <tr>
            <td width="700" align="center" valign="middle"><h3>ICMI 2021 Workshop on</h3>
                <span class="title"><strong>Multimodal Learning with Fewer Samples</strong></span></td>
        </tr>
        <tr>Canada, October 18-22nd, 2021
        <td colspan="3" align="center"><h3>Montreal, Canada<br>8:30 AM - 5:30 PM  on October 21, 2021<br><br>
<!--        <a href="https://zoom.us/j/95655960109?pwd=U3BNbVJ1M0RLYlBodjM5YThaVFlXZz09">[Zoom Meeting]</a> &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp  <a href="https://app.sli.do/event/0iocoe6a">[Ask & Vote Panel Questions]</a>-->
        </tr>
    </table>
    <br><p><img src="ICMI.png" width="1000" align="middle"></p>
    <!--    <p><img src="figures/main.png" width="1000" align="middle"/></p>-->
</div>

</br>

<div class="container">
    <h2>Overview</h2>
    <div class="overview">
        <p>Deep learning has shown remarkable success in multimodal learning where multiple modalities, including linguistic, acoustic and visual messages, are integrating and modeling to come up with a decision. However, deep learning models heavily rely on a huge amount of training data, which is hard to learn novel concepts from a few samples or fast generalizing to new tasks unlike human intelligence. Extensive works have been conducted to overcome the above limitations and address the inefficiency of learning models. Specifically, data from multiple sources are semantically correlated and sometimes provide complementary information to each other, which could compensate for the lack of data and help improve the robust predictions. These works could help us study the learning behaviors of learning models, and bridge the gap between human and machine intelligence. </p>

        <p>In this workshop, we aim to bring together researchers from the fields of zero/few-shot learning, model generalization against OOD samples, and knowledge transfer to discuss recent research and future directions for deep learning models, with a particular focus on multimodal applications. In particular, the topic of multimodal learning with fewer samples is significantly hot and has drawn intensive attention for researchers, which is a more practical, meaningful, and attractive scenario. Therefore, the topic of our proposed workshop is highly important, and we hope will be held annually in conjunction with ICMI.</p>
    </div>
</div>

</br>


<!--<div class="container">-->
<!--    <h2>Important Dates</h2>-->
<!--    TBA-->
<!--</div>-->

<!--</br>-->

<div class="container">
    <h2>Call For Papers</h2>
    <div class="call4papers">
      <p><strong>Submission deadline</strong>: June 8, 2021 Anywhere on Earth (AoE)</p>
        <p><strong>Reviews due</strong>: July 1, 2021 Anywhere on Earth (AoE)</p>
        <p><strong>Notification sent to authors</strong>: July 18, 2021 Anywhere on Earth (AoE)</p>
        <p><strong>Camera ready deadline</strong>: July 25, 2021 Anywhere on Earth (AoE)</p>
        <p><strong>Number of expected papers</strong>: 15</p>
        <p><strong>Submission server</strong>: <a href="https://cmt3.research.microsoft.com/xxx">https://cmt3.research.microsoft.com/xxx</a></p>
        <p><strong>Submission format</strong>: Paper submissions must conform with the “double-blind” review policy. All papers will be peer-reviewed by experts in the field, they will receive at least two reviews. Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality. Based on the PC recommendations, the accepted long papers/extended abstracts will be allocated either a contributed talk or a poster presentation. The workshop papers will be published in the ACM Digital Library.</p>

        <p>We invite submissions on <strong>any aspect of robust multimodal learning with fewer samples</strong>. This includes, but is not limited to:</p>
        <ul>
            <li>New methods for few-/zero-shot learning</li>
            <li>Meta-learning methods</li>
            <li>Novel domain adaptation methods</li>
            <li>Life-long/continual/incremental learning methods</li>
            <li>New methods for transfer learning</li>
            <li>Benchmark for evaluating model generalization</li>
            <li>Understanding the generalization vulnerabilities of deep learning systems</li>
            <li>Improving generalization performance of computer vision systems to out-of-distribution samples (OOD)</li>
   </div>
</div>

<div class="container">
    <h2>Schedule</h2>
<!--    <h3><strong>Please <a href="https://app.sli.do/event/0iocoe6a">ask and vote panel questions</a> ahead</strong></h3>-->
    <div class="schedule">
        <p><strong>08:30 - 08:40 &nbsp &nbsp &nbsp &nbsp  Opening Remark</strong></p>
        <p><strong>08:40 - 09:20 &nbsp  &nbsp &nbsp &nbsp Invited Talk 1:</strong></p>
        <p><strong>09:25 - 10:05 &nbsp  &nbsp &nbsp &nbsp Invited Talk 2:</strong></p>
        <p><strong>10:10 - 11:00 &nbsp &nbsp  &nbsp &nbsp Oral Session 1 (5 selected papers)</strong></p>
        <p><strong>11:00 - 12:30 &nbsp &nbsp  &nbsp &nbsp Poster Session 1</strong></p>
        <p><strong>12:30 - 13:30 &nbsp &nbsp  &nbsp &nbsp Lunch Break</a></strong></p>
        <p><strong>13:30 - 14:20 &nbsp &nbsp  &nbsp &nbsp Invited Talk 3</strong></p>
        <p><strong>14:25 - 15:05 &nbsp &nbsp  &nbsp &nbsp Invited Talk 4</strong></p>
        <p><strong>15:10 - 16:00 &nbsp &nbsp  &nbsp &nbsp Oral Session 2 (5 selected papers)</strong></p>
        <p><strong>16:00 - 17:20 &nbsp &nbsp  &nbsp &nbsp Poster Session 2</strong></p>
        <p><strong>17:20 - 17:30 &nbsp &nbsp  &nbsp &nbsp Closing Remark</strong></p>
    </div>
</div>

<div class="container">
    <h2>Organizing Committee</h2>
    <div>

        <div class="General Chairs">
    <h3>General Chairs</h3>
            <ul>
                <li>Edwin Hancock, the University of York</li>
            </ul>
        </div>
        <div class="Program Chairs">
    <h3>Program Chairs</h3>
            <ul>
                <li>Yuqing Ma, Beihang University</li>
                <li>Aishan Liu, Beihang University</li>
                <li>Huiyuan Xie, Cambridge University</li>
                <li>Yibing Zhan, JD Explore Academy</li>
            </ul>
        </div>
    </div>
</div>
</br>


<div class="container">
    <h2>Speakers</h2>
    <div>
        <div class="instructor">
            <ul>
                <li>Yingwei Li, John Hopkins University </li>
 <li>Cheng Deng, Xidian University</li>
 <li>Ji zhong, Tianjin University</li>
 <li>Xiantong Zhen, IIAI</li>
            </ul>

        </div>
  
    </div>
</div>
</br>



<div class="container">
    <h2>Program Committee</h2>
    <div class="pcs-row pcs">
        <div class="pcs-column">
            <ul>
                <li>Xiaowei Zhao, Beihang University</li>
                <li>Hanghao Wu, Tencent Holdings Ltd</li>
                <li>Hongyang Tang, Shanghai Jiao tong University</li>
                <li>Haotong Qin，Beihang University</li>
                <li>De Xie, Xidian University</li>
            </ul>
        </div>
    </div>
</div>


<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
